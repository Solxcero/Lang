import streamlit as st
from io import BytesIO
from dotenv import load_dotenv
import os

# âœ… ìµœì‹  import ê¶Œì¥
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import ChatMessage, HumanMessage, SystemMessage
from langchain.callbacks.base import BaseCallbackHandler
from langchain_community.vectorstores import FAISS  # (FAISSëŠ” ì»¤ë®¤ë‹ˆí‹° ëª¨ë“ˆ ìœ ì§€)
from pdfminer.high_level import extract_text

load_dotenv()
api_key = os.environ["OPENAI_API_KEY"]

# âœ… ë‹¤êµ­ì–´ ì„±ëŠ¥ ì¢‹ì€ ì„ë² ë”© ì§€ì •
embedding_model = OpenAIEmbeddings(model="text-embedding-3-large")

class MarkdownStreamHandler(BaseCallbackHandler):
    def __init__(self, output_container, initial_content=""):
        self.output_container = output_container
        self.generated_content = initial_content
    def on_llm_new_token(self, token: str, **kwargs) -> None:
        self.generated_content += token
        self.output_container.markdown(self.generated_content)

def extract_text_from_pdf(file):
    try:
        # âœ… UploadedFileì„ ì•ˆì „í•˜ê²Œ BytesIOë¡œ ë³€í™˜
        data = file.read()
        file.seek(0)  # í˜¹ì‹œ ì´í›„ ì°¸ì¡° ëŒ€ë¹„
        return extract_text(BytesIO(data))
    except Exception as error:
        st.error(f"PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {error}")
        return ""

def handle_uploaded_file(file):
    if not file:
        return None, None

    document_text = extract_text_from_pdf(file) if file.type == 'application/pdf' else ""
    if not document_text:
        st.error("í…ìŠ¤íŠ¸ ì¶”ì¶œ ë¶ˆê°€")
        return None, None

    # âœ… ì¡°ê¸ˆ ë” ê²¬ê³ í•œ Splitter
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000, chunk_overlap=200, separators=["\n\n", "\n", " ", ""]
    )
    document_chunks = text_splitter.create_documents([document_text])
    st.info(f"{len(document_chunks)} ê°œì˜ ë¬¸ì„œ ë‹¨ë½ ìƒì„±.")

    vectorstore = FAISS.from_documents(document_chunks, embedding_model)
    return vectorstore, document_text

def get_rag_response(user_query, vectorstore, callback_handler):
    if not vectorstore:
        st.error("No Vectorstore. Upload docs first.")
        return ""

    # âœ… ë‹¤êµ­ì–´ ì„ë² ë”©ì´ë©´ ê·¸ëŒ€ë¡œ ê²€ìƒ‰í•´ë„ ì¢‹ìŠµë‹ˆë‹¤.
    #   (ì •ë°€ë„ë¥¼ ë” ë†’ì´ê³  ì‹¶ìœ¼ë©´ ì•„ë˜ ì£¼ì„ì²˜ëŸ¼ ì˜ì–´ ë²ˆì—­ í›„ ê²€ìƒ‰ ë¡œì§ ì¶”ê°€)
    # eng_query = translate_to_english(user_query)  # ì„ íƒ(ì•„ë˜ ì°¸ê³ )
    retrieved_docs = vectorstore.similarity_search(user_query, k=3)
    retrieved_text = "\n".join(
        f"ë¬¸ì„œ {i+1} : {doc.page_content}" for i, doc in enumerate(retrieved_docs)
    )

    # âœ… ìµœì‹  signature: model=, ê·¸ë¦¬ê³  í•œêµ­ì–´ ê³ ì • ì§€ì‹œ ì¶”ê°€
    chat_model = ChatOpenAI(model="gpt-4o-mini", temperature=0, streaming=True, callbacks=[callback_handler])

    rag_prompt = [
        SystemMessage(content=(
            "ë‹¹ì‹ ì€ ë¬¸ì„œ ê¸°ë°˜ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. "
            "ì˜¤ì§ ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ì„ ê·¼ê±°ë¡œ ë‹µë³€í•˜ì„¸ìš”. "
            "ì •ë³´ê°€ ì—†ìœ¼ë©´ 'ëª¨ë¥´ê² ìŠµë‹ˆë‹¤'ë¼ê³  ë‹µí•˜ì„¸ìš”. "
            "í•­ìƒ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”."
        )),
        HumanMessage(content=f"ì§ˆë¬¸: {user_query}\n\nì°¸ê³  ë¬¸ì„œ ë°œì·Œ:\n{retrieved_text}")
    ]

    try:
        response = chat_model(rag_prompt)
        return response.content
    except Exception as error:
        st.error(f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {error}")
        return ""

st.set_page_config(page_title="My Doc's QnA")
st.title("ğŸ“‚ ë¬¸ì„œ ê¸°ë°˜ QnA ğŸ’¬")

if "vectorstore" not in st.session_state:
    st.session_state["vectorstore"] = None

if "chat_history" not in st.session_state:
    st.session_state["chat_history"] = [
        ChatMessage(role="assistant", content="ì•ˆë…•í•˜ì„¸ìš”, ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”. (ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì œê³µë©ë‹ˆë‹¤)")
    ]

uploaded_file = st.file_uploader("ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”(PDF):", type=["pdf"])
if uploaded_file and uploaded_file != st.session_state.get("uploaded_file"):
    vectorstore, document_text = handle_uploaded_file(uploaded_file)
    if vectorstore:
        st.session_state["vectorstore"] = vectorstore
        st.session_state["uploaded_file"] = uploaded_file

chat_container = st.container()
with chat_container:
    for message in st.session_state.chat_history:
        st.chat_message(message.role).write(message.content)

if user_query := st.chat_input("ì—…ë¡œë“œëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸í•˜ì„¸ìš” (í•œêµ­ì–´ë¡œ ì§ˆë¬¸í•˜ì„¸ìš”)"):
    st.session_state.chat_history.append(ChatMessage(role="user", content=user_query))
    with chat_container:
        st.chat_message("user").write(user_query)

    with st.chat_message("assistant"):
        stream_output = MarkdownStreamHandler(st.empty())
        assistant_response = get_rag_response(user_query, st.session_state.get("vectorstore"), stream_output)
        if assistant_response:
            st.session_state.chat_history.append(ChatMessage(role="assistant", content=assistant_response))
