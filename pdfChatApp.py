import streamlit as st
from langchain_community.chat_models import ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.schema import ChatMessage, HumanMessage, SystemMessage
from langchain.callbacks.base  import BaseCallbackHandler
from pdfminer.high_level import extract_text

import os
from dotenv import load_dotenv


load_dotenv()
api_key = os.environ["OPENAI_API_KEY"] 

embedding_model = OpenAIEmbeddings()

class MarkdownStreamHandler(BaseCallbackHandler):

    '''Streamlit ë§ˆí¬ë‹¤ìš´ ì»¨í…Œì´ë„ˆì— ìƒì„±ëœ í† í°ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë°í•˜ëŠ” ì‚¬ìš©ì ì •ì˜ í•¸ë“¤ëŸ¬'''

    def __init__(self, output_container, initial_content=""):
        self.output_container = output_container
        self.generated_content = initial_content

    def on_llm_new_token(self, token: str, **kwargs) -> None:
        self.generated_content += token
        self.output_container.markdown(self.generated_content)

def extract_text_from_pdf(file):
    try:
        return extract_text(file)
    except Exception as error:
        st.error(f"PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {error}")
        return ""
    

def handle_uploaded_file(file):
    '''ì—…ë¡œë“œ ëœ PDF íŒŒì¼ì„ ì²˜ë¦¬í•˜ê³  ë²¡í„° ìŠ¤í† ì–´ ì¤€ë¹„'''
    if not file:
        return None, None
    
    document_text = extract_text_from_pdf(file) if file.type == 'application/pdf' else ""
    if not document_text:
        st.error("í…ìŠ¤íŠ¸ ì¶”ì¶œ ë¶ˆê°€")
        return None, None
    
    text_splitter = CharacterTextSplitter(
        separator="\n",
        chunk_size=1000,
        chunk_overlap=200
    )

    document_chunks = text_splitter.create_documents([document_text])
    st.info(f"{len(document_chunks)} ê°œì˜ ë¬¸ì„œ ë‹¨ë½ ìƒì„±.")

    # ìœ ì‚¬ì„± ê²€ìƒ‰ì„ ìœ„í•œ ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
    vectorstore = FAISS.from_documents(document_chunks, embedding_model)
    return vectorstore, document_text

def get_rag_response(user_query, vectorstore, callback_handler):
    if not vectorstore:
        st.error("No Vectorstore. Upload docs first.")
        return ""
    
    retrieved_docs = vectorstore.similarity_search(user_query, k=3)
    retrieved_text = "\n".join(f"ë¬¸ì„œ {i+1} : {doc.page_content}" for i, doc in enumerate(retrieved_docs))

    chat_model = ChatOpenAI(model_name = "gpt-4", temperature=0, streaming=True, callbacks=[callback_handler])

    rag_prompt = [
        SystemMessage(content = "ì œê³µëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”. ì •ë³´ê°€ ì—†ìœ¼ë©´ 'ëª¨ë¥´ê² ìŠµë‹ˆë‹¤' ë¼ê³  ë‹µë³€í•˜ì„¸ìš”"),
        HumanMessage(content=f"ì§ˆë¬¸ : {user_query}\n\n{retrieved_text}")

    ]

    try:
        response = chat_model(rag_prompt)
        return response.content
    
    except Exception as error:
        st.error(f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {error}")
        return ""
    

st.set_page_config(page_title="My Doc's QnA")
st.title("ğŸ“‚ ë¬¸ì„œ ê¸°ë°˜ QnA ğŸ’¬")

if "vectorstore" not in st.session_state:
    st.session_state["vectorstore"] = None

if "chat_history" not in st.session_state:
    st.session_state["chat_history"] = [
        ChatMessage(role="assistant", content="ì•ˆë…•í•˜ì„¸ìš”, ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”.")
    ]

uploaded_file = st.file_uploader("ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”(PDF):", type=["pdf"])
if uploaded_file and uploaded_file != st.session_state.get("uploaded_file"):
    vectorstore, document_text = handle_uploaded_file(uploaded_file)
    if vectorstore:
        st.session_state["vectorstore"] = vectorstore
        st.session_state["uploaded_file"] = uploaded_file

chat_container = st.container()
with chat_container:
    for message in st.session_state.chat_history:
        st.chat_message(message.role).write(message.content)

if user_query := st.chat_input("ì—…ë¡œë“œëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸í•˜ì„¸ìš”"):
    st.session_state.chat_history.append(ChatMessage(role="user", content=user_query))

    with chat_container:
        st.chat_message("user").write(user_query)

    
    with st.chat_message("assistant"):
        stream_output = MarkdownStreamHandler(st.empty())
        assistant_response = get_rag_response(user_query, st.session_state.get("vectorstore"), stream_output)

        if assistant_response:
            st.session_state.chat_history.append(ChatMessage(role="assistant", content=assistant_response))
